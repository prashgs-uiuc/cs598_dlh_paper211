{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec, FastText\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_of_ner = \"new\"\n",
    "\n",
    "x_train_lstm = pd.read_pickle(\"data/\"+type_of_ner+\"_x_train.pkl\")\n",
    "x_dev_lstm = pd.read_pickle(\"data/\"+type_of_ner+\"_x_dev.pkl\")\n",
    "x_test_lstm = pd.read_pickle(\"data/\"+type_of_ner+\"_x_test.pkl\")\n",
    "\n",
    "y_train = pd.read_pickle(\"data/\"+type_of_ner+\"_y_train.pkl\")\n",
    "y_dev = pd.read_pickle(\"data/\"+type_of_ner+\"_y_dev.pkl\")\n",
    "y_test = pd.read_pickle(\"data/\"+type_of_ner+\"_y_test.pkl\")\n",
    "\n",
    "\n",
    "ner_word2vec = pd.read_pickle(\"data/\"+type_of_ner+\"_ner_word2vec_limited_dict.pkl\")\n",
    "ner_fasttext = pd.read_pickle(\"data/\"+type_of_ner+\"_ner_fasttext_limited_dict.pkl\")\n",
    "ner_concat = pd.read_pickle(\"data/\"+type_of_ner+\"_ner_combined_limited_dict.pkl\")\n",
    "\n",
    "train_ids = pd.read_pickle(\"data/\"+type_of_ner+\"_train_ids.pkl\")\n",
    "dev_ids = pd.read_pickle(\"data/\"+type_of_ner+\"_dev_ids.pkl\")\n",
    "test_ids = pd.read_pickle(\"data/\"+type_of_ner+\"_test_ids.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, x_train, y_train):\n",
    "        self.x = x_train\n",
    "        self.y = y_train\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: Return the number of samples (i.e. patients).\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: Generates one sample of data.\n",
    "        \n",
    "        Note that you DO NOT need to covert them to tensor as we will do this later.\n",
    "        \"\"\"\n",
    "        \n",
    "        # your code here\n",
    "        return self.x[index], self.y.iloc[[index]].values\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    x, y = zip(*data)\n",
    "    x = torch.Tensor(x)\n",
    "    y = torch.Tensor(y)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = CustomDataset(x_train_lstm, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, collate_fn=collate_fn)\n",
    "\n",
    "# for x,y in train_loader:\n",
    "#       print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (conv1): Conv1d(24, 104, kernel_size=(2,), stride=(1,))\n",
      "  (conv2): Conv1d(104, 64, kernel_size=(3,), stride=(1,))\n",
      "  (conv3): Conv1d(104, 96, kernel_size=(4,), stride=(1,))\n",
      "  (rnn): GRU(50, 256)\n",
      "  (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=7, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_of_units,\n",
    "        ner_limit,\n",
    "        num_embeddings,\n",
    "        embedding_dim, \n",
    "        kernel_sizes, \n",
    "        num_filters, \n",
    "        num_classes=4, d_prob=0.5):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=24, out_channels=104, kernel_size=kernel_sizes[0], stride=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=104, out_channels=num_filters*2, kernel_size=kernel_sizes[1], stride=1)\n",
    "        self.conv3 = nn.Conv1d(in_channels=embedding_dim, out_channels=num_filters*3, kernel_size=kernel_sizes[2], stride=1)\n",
    "        self.rnn = nn.GRU(input_size=50, hidden_size=256, batch_first= False)\n",
    "        self.fc1 = nn.Linear(in_features=256, out_features=256)\n",
    "        self.fc2 = nn.Linear(in_features=256, out_features=7)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        print(f'input-{x.shape}')\n",
    "        x = F.relu(self.conv1(x))\n",
    "        print(f'first conv-{x.shape}')\n",
    "        x = F.relu(self.conv2(x))\n",
    "        print(f'second conv-{x.shape}')\n",
    "        m = nn.MaxPool1d(2)\n",
    "        x = m(x)\n",
    "        print(f'max pool-{x.shape}')\n",
    "        output, _ = self.rnn(x)\n",
    "        print(f'rnn output-{output.shape}')\n",
    "        concat = torch.cat([x, output], dim=0)\n",
    "        print(f'concat-{concat.shape}')\n",
    "        concat = self.fc1(torch.cat([x, output], 1))\n",
    "        concat = F.relu(concat)\n",
    "        concat = self.dropout(concat)\n",
    "        output = F.sigmoid(self.fc2(concat))\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "model = Model(num_of_units=256, ner_limit=64, num_embeddings=24, embedding_dim=104, kernel_sizes=[2, 3, 4], num_filters=32, num_classes=4, d_prob=0.5)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TextCNN(nn.Module):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         num_embeddings,\n",
    "#         embedding_dim, \n",
    "#         kernel_sizes, \n",
    "#         num_filters, \n",
    "#         num_classes=4, d_prob=0.5):\n",
    "#         super(TextCNN, self).__init__()\n",
    "#         self.num_embeddings = num_embeddings\n",
    "#         self.embedding_dim = embedding_dim\n",
    "#         self.kernel_sizes = kernel_sizes\n",
    "#         self.num_filters = num_filters\n",
    "#         self.num_classes = num_classes\n",
    "#         self.d_prob = d_prob\n",
    "#         self.embedding = nn.Embedding(num_embeddings=num_embeddings, embedding_dim=embedding_dim)\n",
    "#         self.conv = nn.ModuleList([nn.Conv1d(in_channels=embedding_dim,\n",
    "#                                              out_channels=num_filters,\n",
    "#                                              kernel_size=k, stride=1) for k in kernel_sizes])\n",
    "#         self.dropout = nn.Dropout(d_prob)\n",
    "#         self.fc = nn.Linear(len(kernel_sizes) * num_filters, num_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.embedding(x.T).transpose(1, 2)\n",
    "#         x = [F.relu(conv(x)) for conv in self.conv]\n",
    "#         x = [F.max_pool1d(c, c.size(-1)).squeeze(dim=-1) for c in x]\n",
    "#         x = torch.cat(x, dim=1)\n",
    "#         x = self.fc(self.dropout(x))\n",
    "#         return torch.sigmoid(x).squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = TextCNN(num_embeddings=24,\n",
    "#                 embedding_dim=104,\n",
    "#                 kernel_sizes=[2, 3, 4],\n",
    "#                 num_filters=32,\n",
    "#                 num_classes=4, \n",
    "#                 d_prob=0.5)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-3)\n",
    "# criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TODO: Specify loss function (CrossEntropyLoss) and assign it to `criterion`.\n",
    "Spcify optimizer (SGD) and assign it to `optimizer`.\n",
    "Hint: the learning rate is usually a small number on the scale of 1e-4 ~ 1e-2\n",
    "\"\"\"\n",
    "\n",
    "criterion = None\n",
    "optimizer = None\n",
    "\n",
    "# your code here\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# def load_data(train_data, val_data):\n",
    "#     train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "#     val_loader = torch.utils.data.DataLoader(val_data, batch_size=64, shuffle=False)\n",
    "#     return train_loader, val_loader\n",
    "\n",
    "\n",
    "# train_loader, val_loader = load_data(x_train_lstm, y_train)\n",
    "# for batch_idx, samples in enumerate(zip(train_loader, val_loader)):\n",
    "#       print(batch_idx, samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_subvector_data(size, embed_name, data):\n",
    "#     if embed_name == \"concat\":\n",
    "#         vector_size = 200\n",
    "#     else:\n",
    "#         vector_size = 100\n",
    "\n",
    "#     x_data = {}\n",
    "#     for k, v in data.items():\n",
    "#         number_of_additional_vector = len(v) - size\n",
    "#         vector = []\n",
    "#         for i in v:\n",
    "#             vector.append(i)\n",
    "#         if number_of_additional_vector < 0: \n",
    "#             number_of_additional_vector = np.abs(number_of_additional_vector)\n",
    "\n",
    "#             temp = vector[:size]\n",
    "#             for i in range(0, number_of_additional_vector):\n",
    "#                 temp.append(np.zeros(vector_size))\n",
    "#             x_data[k] = np.asarray(temp)\n",
    "#         else:\n",
    "#             x_data[k] = np.asarray(vector[:size])\n",
    "\n",
    "#     return x_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch = 100\n",
    "model_patience = 5\n",
    "monitor_criteria = 'val_loss'\n",
    "#monitor_criteria = 'val_acc'\n",
    "batch_size = 64\n",
    "\n",
    "filter_number = 32\n",
    "ner_representation_limit = 64\n",
    "activation_func = \"relu\"\n",
    "\n",
    "sequence_model = \"GRU\"\n",
    "sequence_hidden_unit = 256\n",
    "\n",
    "# x_train_dict = get_subvector_data(ner_representation_limit, embed_name, temp_train_ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of epochs to train the model\n",
    "# make sure your model finish training within 4 minutes on a CPU machine\n",
    "# You can experiment different numbers for n_epochs, but even 1 epoch should be good enough.\n",
    "n_epochs = 1\n",
    "\n",
    "def train_model(model, train_dataloader, n_epoch=n_epochs, optimizer=optimizer, criterion=criterion):\n",
    "    import torch.optim as optim\n",
    "    \"\"\"\n",
    "    :param model: A CNN model\n",
    "    :param train_dataloader: the DataLoader of the training data\n",
    "    :param n_epoch: number of epochs to train\n",
    "    :return:\n",
    "        model: trained model\n",
    "    \"\"\"\n",
    "    model.train() # prep model for training\n",
    "\n",
    "    \n",
    "    for epoch in range(n_epoch):\n",
    "        curr_epoch_loss = []\n",
    "        for x, y in train_dataloader:\n",
    "            \"\"\"\n",
    "            TODO: Within the loop, do the normal training procedures:\n",
    "                   pass the input through the model\n",
    "                   pass the output through loss_func to compute the loss (name the variable as *loss*)\n",
    "                   zero out currently accumulated gradient, use loss.basckward to backprop the gradients, then call optimizer.step\n",
    "            \"\"\"\n",
    "            # your code here\n",
    "            print(x.shape, y.shape)\n",
    "            y_hat = None\n",
    "            loss = None\n",
    "            y_hat = model(x)\n",
    "            y_hat = torch.squeeze(y_hat, 1)\n",
    "            loss = criterion(y_hat,y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            curr_epoch_loss.append(loss.cpu().data.numpy())\n",
    "        print(f\"Epoch {epoch}: curr_epoch_loss={np.mean(curr_epoch_loss)}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 24, 104]) torch.Size([64, 1, 7])\n",
      "input-torch.Size([64, 24, 104])\n",
      "first conv-torch.Size([64, 104, 103])\n",
      "second conv-torch.Size([64, 64, 101])\n",
      "max pool-torch.Size([64, 64, 50])\n",
      "rnn output-torch.Size([64, 64, 256])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 0. Expected size 50 but got size 256 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/media/prashanth/Learning/mcs-ds/CS598_DLH/CS598_DLH/Project/Paper211/ConvolutionMedicalNer/09-Proposed-Model-pytorch.ipynb Cell 16'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/media/prashanth/Learning/mcs-ds/CS598_DLH/CS598_DLH/Project/Paper211/ConvolutionMedicalNer/09-Proposed-Model-pytorch.ipynb#ch0000016?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m train_model(model, train_loader)\n",
      "\u001b[1;32m/media/prashanth/Learning/mcs-ds/CS598_DLH/CS598_DLH/Project/Paper211/ConvolutionMedicalNer/09-Proposed-Model-pytorch.ipynb Cell 15'\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dataloader, n_epoch, optimizer, criterion)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/prashanth/Learning/mcs-ds/CS598_DLH/CS598_DLH/Project/Paper211/ConvolutionMedicalNer/09-Proposed-Model-pytorch.ipynb#ch0000015?line=28'>29</a>\u001b[0m y_hat \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/prashanth/Learning/mcs-ds/CS598_DLH/CS598_DLH/Project/Paper211/ConvolutionMedicalNer/09-Proposed-Model-pytorch.ipynb#ch0000015?line=29'>30</a>\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/media/prashanth/Learning/mcs-ds/CS598_DLH/CS598_DLH/Project/Paper211/ConvolutionMedicalNer/09-Proposed-Model-pytorch.ipynb#ch0000015?line=30'>31</a>\u001b[0m y_hat \u001b[39m=\u001b[39m model(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/prashanth/Learning/mcs-ds/CS598_DLH/CS598_DLH/Project/Paper211/ConvolutionMedicalNer/09-Proposed-Model-pytorch.ipynb#ch0000015?line=31'>32</a>\u001b[0m y_hat \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msqueeze(y_hat, \u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/prashanth/Learning/mcs-ds/CS598_DLH/CS598_DLH/Project/Paper211/ConvolutionMedicalNer/09-Proposed-Model-pytorch.ipynb#ch0000015?line=32'>33</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(y_hat,y)\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/prashanth/.conda/envs/pytorch_env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/prashanth/.conda/envs/pytorch_env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/prashanth/.conda/envs/pytorch_env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/prashanth/.conda/envs/pytorch_env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/prashanth/.conda/envs/pytorch_env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/prashanth/.conda/envs/pytorch_env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/prashanth/.conda/envs/pytorch_env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/media/prashanth/Learning/mcs-ds/CS598_DLH/CS598_DLH/Project/Paper211/ConvolutionMedicalNer/09-Proposed-Model-pytorch.ipynb Cell 8'\u001b[0m in \u001b[0;36mModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/prashanth/Learning/mcs-ds/CS598_DLH/CS598_DLH/Project/Paper211/ConvolutionMedicalNer/09-Proposed-Model-pytorch.ipynb#ch0000006?line=31'>32</a>\u001b[0m output, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrnn(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/prashanth/Learning/mcs-ds/CS598_DLH/CS598_DLH/Project/Paper211/ConvolutionMedicalNer/09-Proposed-Model-pytorch.ipynb#ch0000006?line=32'>33</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrnn output-\u001b[39m\u001b[39m{\u001b[39;00moutput\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/media/prashanth/Learning/mcs-ds/CS598_DLH/CS598_DLH/Project/Paper211/ConvolutionMedicalNer/09-Proposed-Model-pytorch.ipynb#ch0000006?line=33'>34</a>\u001b[0m concat \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat([x, output], dim\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/prashanth/Learning/mcs-ds/CS598_DLH/CS598_DLH/Project/Paper211/ConvolutionMedicalNer/09-Proposed-Model-pytorch.ipynb#ch0000006?line=34'>35</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mconcat-\u001b[39m\u001b[39m{\u001b[39;00mconcat\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/prashanth/Learning/mcs-ds/CS598_DLH/CS598_DLH/Project/Paper211/ConvolutionMedicalNer/09-Proposed-Model-pytorch.ipynb#ch0000006?line=35'>36</a>\u001b[0m concat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc1(torch\u001b[39m.\u001b[39mcat([x, output], \u001b[39m1\u001b[39m))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 0. Expected size 50 but got size 256 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "\n",
    "model = train_model(model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, dataloader):\n",
    "    \"\"\"\n",
    "    :return:\n",
    "        Y_pred: prediction of model on the dataloder.\n",
    "            Should be an 2D numpy float array where the second dimension has length 2.\n",
    "        Y_test: truth labels. Should be an numpy array of ints\n",
    "    TODO:\n",
    "        evaluate the model using on the data in the dataloder.\n",
    "        Add all the prediction and truth to the corresponding list\n",
    "        Convert Y_pred and Y_test to numpy arrays (of shape (n_data_points, 2))\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    Y_pred = []\n",
    "    Y_test = []\n",
    "    for data, target in dataloader:\n",
    "        # your code here\n",
    "        y_hat = model(data)\n",
    "        y_hat = torch.argmax(y_hat, dim=1)\n",
    "        y_pred = y_hat.detach().numpy()\n",
    "        target = target.detach().numpy()\n",
    "        Y_pred.append(y_pred)\n",
    "        Y_test.append(target)\n",
    "    \n",
    "    Y_pred = np.concatenate(Y_pred, axis=0)\n",
    "    Y_test = np.concatenate(Y_test, axis=0)\n",
    "\n",
    "    return Y_pred, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
    "'''\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred, y_true = eval_model(model, val_loader)\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "print((\"Validation Accuracy: \" + str(acc)))\n",
    "assert acc > 0.7, \"Validation Accuracy below 0.7 for validation data!\"\n",
    "assert len(y_true) == len(y_pred) == 168, \"Output size is wrong\"\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d5620b39e9068a95f4be31b345ed097523fa5763f9954aba3585c5f534bfd4d6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('rapids-22.04')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
